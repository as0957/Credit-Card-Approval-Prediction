{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectFromModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_data(features):\n",
    "    # Convert column names to strings\n",
    "    features.columns = features.columns.astype(str)\n",
    "    \n",
    "    # Replace '?' with NaN\n",
    "    features = features.replace('?', np.nan)\n",
    "    \n",
    "    # Separate numerical and categorical columns\n",
    "    num_cols = features.select_dtypes(include=[np.number]).columns\n",
    "    cat_cols = features.select_dtypes(exclude=[np.number]).columns\n",
    "    \n",
    "    # Advanced imputation using KNN for numerical columns\n",
    "    imputer = KNNImputer(n_neighbors=5)\n",
    "    if not num_cols.empty:\n",
    "        features[num_cols] = imputer.fit_transform(features[num_cols])\n",
    "    \n",
    "    # Handle categorical variables\n",
    "    for col in cat_cols:\n",
    "        features[col] = features[col].fillna(features[col].mode()[0])\n",
    "        features[col] = pd.factorize(features[col])[0]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n",
    "    \n",
    "    # Ensure column names are strings\n",
    "    features.columns = features.columns.astype(str)\n",
    "    \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def engineer_features(data):\n",
    "    # Convert all column names to strings first\n",
    "    data.columns = data.columns.astype(str)\n",
    "    \n",
    "    # First, let's print the data types to debug\n",
    "    print(\"Column dtypes before feature engineering:\")\n",
    "    print(data.dtypes)\n",
    "    \n",
    "    # Get only numeric columns\n",
    "    num_cols = data.select_dtypes(include=['int64', 'float64']).columns\n",
    "    print(\"\\nNumeric columns selected:\", num_cols.tolist())\n",
    "    \n",
    "    # Create a copy of the data to avoid modifying the original\n",
    "    engineered_data = data.copy()\n",
    "    \n",
    "    # Create interaction features\n",
    "    new_features = {}\n",
    "    for i, col1 in enumerate(num_cols[:-1]):\n",
    "        for j, col2 in enumerate(num_cols[i+1:]):\n",
    "            col_name = f'interaction_{col1}_{col2}'\n",
    "            new_features[col_name] = data[col1].astype(float) * data[col2].astype(float)\n",
    "    \n",
    "    # Add interaction features\n",
    "    if new_features:\n",
    "        interaction_df = pd.DataFrame(new_features)\n",
    "        engineered_data = pd.concat([engineered_data, interaction_df], axis=1)\n",
    "    \n",
    "    # Add polynomial features\n",
    "    poly_features = {}\n",
    "    for col in num_cols:\n",
    "        # Square term\n",
    "        poly_features[f'{col}_squared'] = data[col].astype(float) ** 2\n",
    "        # Cube term\n",
    "        poly_features[f'{col}_cubed'] = data[col].astype(float) ** 3\n",
    "    \n",
    "    # Add polynomial features\n",
    "    if poly_features:\n",
    "        poly_df = pd.DataFrame(poly_features)\n",
    "        engineered_data = pd.concat([engineered_data, poly_df], axis=1)\n",
    "    \n",
    "    # Ensure all column names are strings\n",
    "    engineered_data.columns = engineered_data.columns.astype(str)\n",
    "    \n",
    "    print(\"\\nShape after feature engineering:\", engineered_data.shape)\n",
    "    return engineered_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class StackedEnsembleClassifier:\n",
    "    def __init__(self):\n",
    "        self.base_models = [\n",
    "            GradientBoostingClassifier(n_estimators=100, learning_rate=0.1),\n",
    "            RandomForestClassifier(n_estimators=100),\n",
    "            AdaBoostClassifier(n_estimators=100),\n",
    "            MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500)\n",
    "        ]\n",
    "        self.meta_model = GradientBoostingClassifier(n_estimators=50)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # Train base models\n",
    "        self.meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            model.fit(X, y)\n",
    "            self.meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "        # Train meta model\n",
    "        self.meta_model.fit(self.meta_features, y)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        meta_features = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        \n",
    "        for i, model in enumerate(self.base_models):\n",
    "            meta_features[:, i] = model.predict_proba(X)[:, 1]\n",
    "            \n",
    "        return self.meta_model.predict(meta_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original target values: ['+' '-']\n",
      "After initial conversion: [1 0]\n",
      "After preprocessing target: [1 0]\n",
      "Column dtypes before feature engineering:\n",
      "0     float64\n",
      "1     float64\n",
      "2     float64\n",
      "3     float64\n",
      "4     float64\n",
      "5     float64\n",
      "6     float64\n",
      "7     float64\n",
      "8     float64\n",
      "9     float64\n",
      "10    float64\n",
      "11    float64\n",
      "12    float64\n",
      "13    float64\n",
      "14    float64\n",
      "dtype: object\n",
      "\n",
      "Numeric columns selected: ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14']\n",
      "\n",
      "Shape after feature engineering: (690, 150)\n",
      "After engineering target: [1 0]\n",
      "Final target values: [1 0]\n",
      "Train set target values: [0 1]\n",
      "Test set target values: [0 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anand/STUDY/Credit card approval using ensemble classifier/credit/lib/python3.12/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted target values: [0 1]\n",
      "\n",
      "Accuracy: 0.8985507246376812\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.90        68\n",
      "           1       0.95      0.84      0.89        70\n",
      "\n",
      "    accuracy                           0.90       138\n",
      "   macro avg       0.90      0.90      0.90       138\n",
      "weighted avg       0.90      0.90      0.90       138\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Load data\n",
    "    data = pd.read_csv('credit_approval/crx.data', header=None)\n",
    "    \n",
    "    # Convert column names to strings right after loading\n",
    "    data.columns = data.columns.astype(str)\n",
    "    \n",
    "    # Print original target values\n",
    "    print(\"Original target values:\", data.iloc[:, -1].unique())\n",
    "    \n",
    "    # Convert target to binary first (before any preprocessing)\n",
    "    data.iloc[:, -1] = (data.iloc[:, -1] == '+').astype(int)\n",
    "    print(\"After initial conversion:\", data.iloc[:, -1].unique())\n",
    "    \n",
    "    # Preprocess features only (excluding target)\n",
    "    features = data.iloc[:, :-1].copy()\n",
    "    target = data.iloc[:, -1].copy()\n",
    "    \n",
    "    # Preprocess features\n",
    "    processed_features = preprocess_data(features)\n",
    "    print(\"After preprocessing target:\", target.unique())\n",
    "    \n",
    "    # Engineer features\n",
    "    engineered_features = engineer_features(processed_features)\n",
    "    print(\"After engineering target:\", target.unique())\n",
    "    \n",
    "    # Ensure target remains binary\n",
    "    target = target.astype(int)\n",
    "    print(\"Final target values:\", target.unique())\n",
    "    \n",
    "    # Split into features and target\n",
    "    X = engineered_features\n",
    "    y = target\n",
    "    \n",
    "    # Ensure X has string column names\n",
    "    X.columns = X.columns.astype(str)\n",
    "    \n",
    "    # Feature selection\n",
    "    selector = SelectFromModel(GradientBoostingClassifier())\n",
    "    X_selected = selector.fit_transform(X, y)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_selected, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Verify target values in train and test sets\n",
    "    print(\"Train set target values:\", y_train.unique())\n",
    "    print(\"Test set target values:\", y_test.unique())\n",
    "    \n",
    "    # Train stacked ensemble\n",
    "    model = StackedEnsembleClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"\\nPredicted target values:\", np.unique(y_pred))\n",
    "    print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
